#!/bin/bash

NUM_GPUS=8
MODEL_NAME=DeepSeek-R1-Distill-Qwen-1.5B-GRPO

MODEL=/work/open-r1/data/${MODEL_NAME}
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,data_parallel_size=$NUM_GPUS,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"

OUTPUT_DIR=/work/open-r1/data/evals/${MODEL_NAME}

export VLLM_WORKER_MULTIPROC_METHOD=spawn


LEVELS=("easy" "basic" "medium" "hard")

V_LOL_Benchmark_Suite=""
for LEVEL in "${LEVELS[@]}"; do
  benchmark_entry="custom|V-LOL-Benchmark-Tier-${LEVEL}|0|0"

  if [[ -n "$V_LOL_Benchmark_Suite" ]]; then
    V_LOL_Benchmark_Suite+=",$benchmark_entry"
  else
    V_LOL_Benchmark_Suite="$benchmark_entry"
  fi
done

lighteval vllm $MODEL_ARGS "${V_LOL_Benchmark_Suite}" \
    --custom-tasks /work/open-r1/custom_eval/custom_lighteval_tasks.py \
    --output-dir $OUTPUT_DIR

ALL_BENCHMARKS="lighteval|aime24|0|0,lighteval|aime25|0|0,lighteval|math_500|0|0"

lighteval vllm $MODEL_ARGS ${ALL_BENCHMARKS} \
    --output-dir $OUTPUT_DIR \
    --use-chat-template
